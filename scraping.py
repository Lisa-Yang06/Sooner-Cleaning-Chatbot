from dotenv import load_dotenv #æ¥è‡ªdetenvè¿™ä¸ªåº“ è¯»å–.evné‡Œçš„ç¯å¢ƒå˜é‡
import os
import time, re, json, requests
from collections import deque
from bs4 import BeautifulSoup
import urllib.parse as up
from urllib.robotparser import RobotFileParser

load_dotenv(dotenv_path="/Users/yuxuanyang/soonerbot/.env")
api_key = os.getenv("OPENAI_API_KEY")
#print(api_key)

import requests
from bs4 import BeautifulSoup
RESPECT_ROBOTS = True #ä¸çˆ¬ä¸è®©çˆ¬çš„å†…å®¹
BASE_URL = "https://www.soonercleaning.com/" #é¿å…çˆ¬å‡ºç«™
PATH_ALLOW = {
    "",
    "aboutus.html",
    "products.html",
    "news.html",
    "support.html",
    "contactus.html"
}
ALLOW_DOMAINS = {"soonercleaning.com", "www.soonercleaning.com"}
MAX_PAGES  = 50000
MAX_DEPTH  = 50
REQUEST_INTERVAL = 0.25
TIMEOUT    = 12
HEADERS    = {"User-Agent":"SoonerBot/1.0 (+site QA)"}

def same_site(u: str) -> bool:
    """åŒåŸŸåˆ¤æ–­ï¼šscheme http/https ä¸” netloc åœ¨ç™½åå•å†…"""
    try:
        p = up.urlparse(u)
        return (p.scheme in ("http", "https")) and (p.netloc in ALLOW_DOMAINS)
    except Exception:
        return False

def canonicalize(url: str) -> str:
    # ç»Ÿä¸€urlæ ¼å¼ é¿å…åŒä¸€é¡µé¢å¤šæ¬¡æŠ“å–
    """URL è§„èŒƒåŒ–ï¼šå» #fragmentã€å»å¸¸è§è¿½è¸ªå‚æ•°ã€æ ‡å‡†åŒ–è·¯å¾„/åŸŸåå¤§å°å†™"""

    u = up.urlparse(url.split("#")[0]) #æŠŠé”šç‚¹å»æ‰ è¿™ä¸ªåªå½±å“é¡µé¢å®šä½ ä¸å½±å“å†…å®¹
    qs = up.parse_qs(u.query, keep_blank_values=False)
    keep = {}  # å¦‚æœéœ€è¦ä¿ç•™æŸäº›å‚æ•°ï¼Œåœ¨è¿™é‡Œæ”¾å…¥ï¼ˆå¦‚ keep["lang"] = qs.get("lang", [])ï¼‰
    new_q = up.urlencode(keep, doseq=True)

    path = u.path or "/"
    # ç»Ÿä¸€å»æ‰éæ ¹è·¯å¾„æœ«å°¾çš„æ–œæ 
    if path != "/" and path.endswith("/"):
        path = path[:-1]

    return up.urlunparse((u.scheme, u.netloc.lower(), path, "", new_q, ""))


def extract_text(html: str) -> tuple[str, str]:
    soup = BeautifulSoup(html, "html.parser")
    title = (soup.title.string or "").strip() if (soup.title and soup.title.string) else ""

    # å…ˆé”å®š #content
    content = soup.select_one("#content")

    # å»å™ªï¼Œä½†ä¿ç•™ #content å­æ ‘
    noise_selectors = ("nav", "header", "footer", ".navbar", ".menu", ".sidebar", ".footer", "script", "style")
    for sel in noise_selectors:
        for t in soup.select(sel):
            if content and content in t.parents:  # åœ¨ #content é‡Œå°±è·³è¿‡
                continue
            t.decompose()

    # é‡æ–°è·å–ï¼ˆå»å™ªåç»“æ„æ›´å¹²å‡€ï¼‰
    content = soup.select_one("#content") or content

    blocks = []

    # 1) #content å†… .descï¼ˆå« description/desc-*ï¼‰
    if content:
        for node in content.select(".desc, [class*='desc']"):
            txt = node.get_text(" ", strip=True)
            if txt:
                blocks.append(txt)

        # 2) #content å†…å¸¸è§„æ­£æ–‡
        for tag in content.select("h1, h2, h3, h4, h5, p, li"):
            txt = tag.get_text(" ", strip=True)
            if txt:
                blocks.append(txt)

    # 3) å…¨é¡µè¡¥å……ï¼ˆ#content ä¹‹å¤–çš„é‡è¦æ–‡æœ¬ï¼‰
    for tag in soup.select("h1, h2, h3, h4, h5, p, li"):
        txt = tag.get_text(" ", strip=True)
        if txt:
            blocks.append(txt)

    # å»é‡ä¿åº
    seen, uniq = set(), []
    for b in blocks:
        if b not in seen:
            seen.add(b)
            uniq.append(b)

    text = "\n".join(([title] if title else []) + uniq)
    text = re.sub(r"\n{2,}", "\n", text)
    print(text)
    return title, text


def build_robot(base: str) -> RobotFileParser | None:
    if not RESPECT_ROBOTS: #å¦‚æœä¸éœ€è¦respect roboté‚£å¯ä»¥ç›´æ¥è¿”å›none åé¢å°±ä¸æŸ¥robitsäº†
        return None
    rp = RobotFileParser()
    try:
        rp.set_url(up.urljoin(base, "/robots.txt"))
        rp.read() #ç”¨readè¯»å–robotè§„åˆ™
        return rp
    except Exception:
        return None

READ_MORE_PAT = re.compile(
    r"(Read\s*More|learn\s*more|more\s*details|full\s*article|æŸ¥çœ‹è¯¦æƒ…|äº†è§£æ›´å¤š|é˜…è¯»å…¨æ–‡|æ›´å¤šè¯¦æƒ…|æŸ¥çœ‹å…¨æ–‡)",
    re.I,
)

def safe_href(href: str) -> bool:
    href = href.strip().lower()
    return not (href.startswith(("mailto:", "tel:", "javascript:", "#")))

def is_read_more_anchor(a_tag) -> bool:
    """åˆ¤æ–­ä¸€ä¸ª <a> æ˜¯å¦æ˜¯ 'Read More/æŸ¥çœ‹è¯¦æƒ…' ç±»å‹çš„æŒ‰é’®/é“¾æ¥ã€‚"""
    # 1) æ–‡æœ¬åŒ¹é…
    text = (a_tag.get_text(" ", strip=True) or "").lower()
    if READ_MORE_PAT.search(text):
        return True
    # 2) class/aria-label/rel ç­‰å±æ€§è¾…åŠ©åˆ¤æ–­
    cls = " ".join(a_tag.get("class") or []).lower()
    aria = (a_tag.get("aria-label") or "").lower()
    rel  = " ".join(a_tag.get("rel") or []).lower()
    if any(k in cls for k in ["readmore", "more", "btn-more", "btn_read", "see-more", "detail"]):
        return True
    if READ_MORE_PAT.search(aria) or READ_MORE_PAT.search(rel):
        return True
    # 3) href æœ¬èº«åŒ…å«å…³é”®è¯ï¼ˆæœ‰äº›ç«™ç›´æ¥æŠŠ more æ”¾è·¯å¾„ï¼‰
    href = (a_tag.get("href") or "").lower()
    if any(k in href for k in ["read", "more", "detail", "details"]):
        return True
    return False

def looks_like_html(url: str) -> bool:
    bad_ext = (".pdf",".jpg",".jpeg",".png",".gif",".svg",".webp",".zip",".rar",".7z",
               ".doc",".docx",".xls",".xlsx",".ppt",".pptx",".mp4",".mp3",".avi",".mov",
               ".css",".js",".ico")
    p = up.urlparse(url)
    return not any(p.path.lower().endswith(ext) for ext in bad_ext)

def crawl():
    # 1) èµ·å§‹é˜Ÿåˆ—
    seeds = [canonicalize(up.urljoin(BASE_URL, p)) for p in PATH_ALLOW]
    seeds = list(dict.fromkeys(seeds)) #å»é‡å¤å¹¶ä¿æŒé¡ºåº

    rp = build_robot(BASE_URL)
    seen: set[str] = set()      # å·²æŠ“å–æˆ–ç¡®è®¤å¤„ç†è¿‡
    queued: set[str] = set()    # å·²ç»å…¥é˜Ÿï¼Œé¿å…é‡å¤å…¥é˜Ÿ
    q: deque[tuple[str, int]] = deque() #dequeæ”¯æŒå¤´å°¾å…¥é˜Ÿ

    for u in seeds:
        if same_site(u) and looks_like_html(u):
            q.append((u, 0))
            queued.add(u)

    out: list[dict] = []
    pages_done = 0

    while q and len(out) < MAX_PAGES:
        url, depth = q.popleft()
        print(f"[crawl] depth={depth} queue={len(q)} url={url}")

        if depth > MAX_DEPTH:
            continue

        url = canonicalize(url)

        # robots æ£€æŸ¥
        if rp and RESPECT_ROBOTS:
            try:
                if not rp.can_fetch(HEADERS["User-Agent"], url):
                    continue
            except Exception:
                pass

        # æŠ“é¡µé¢
        try:
            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        except requests.RequestException:
            continue
        # ä¸‹é¢è¿™ä¸¤è¡Œæ˜¯åœ¨è¿‡æ»¤æ‰ä¸æ­£å¸¸çš„htmlå“åº”
        ctype = (r.headers.get("Content-Type") or "").lower()
        if r.status_code != 200 or ("text/html" not in ctype and "application/xhtml+xml" not in ctype):
            continue

        # æŠ½æ­£æ–‡
        title, text = extract_text(r.text)
        if len(text) >= 120:
            out.append({"url": url, "title": title, "text": text})
            pages_done += 1
            if pages_done % 50 == 0:
                print(f"[progress] saved={pages_done}, total_out={len(out)}")

        # æ ‡è®°å·²å¤„ç†ï¼ˆé¿å…é‡å¤æŠ“ï¼‰
        seen.add(url)

        # è§£æé“¾æ¥ï¼šå…ˆåˆ†ç±»ï¼Œç»ä¸åœ¨è¿™ä¹‹å‰å…¥é˜Ÿ
        # ä¸‹é¢è¿™ä¸ªåº”è¯¥éƒ½æ˜¯åœ¨å¤„ç†href
        soup = BeautifulSoup(r.text, "html.parser")
        read_more_links = []
        normal_links = []

        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if not safe_href(href):
                continue

            nxt = up.urljoin(url, href)
            nxt = canonicalize(nxt)

            if not same_site(nxt):
                continue
            if not looks_like_html(nxt):
                continue
            if nxt in seen or nxt in queued:
                continue

            if is_read_more_anchor(a):
                read_more_links.append(nxt)
            else:
                normal_links.append(nxt)

        # **ç­–ç•¥ï¼šRead More ä¼˜å…ˆå…¥é˜Ÿ**ï¼ˆä¸ç›´æ¥æŠ“ï¼Œé¿å…å½“å‰é¡µè§¦å‘å¤§é‡è¯·æ±‚ï¼‰
        for detail_url in read_more_links:
            q.appendleft((detail_url, depth + 1)) #append leftä¼˜å…ˆå¤„ç†
            queued.add(detail_url)

        # å…¶æ¬¡å†å…¥é˜Ÿæ™®é€šé“¾æ¥
        for nxt in normal_links:
            q.append((nxt, depth + 1))
            queued.add(nxt)

        time.sleep(REQUEST_INTERVAL)

    return out

if __name__ == "__main__":
    os.makedirs("data", exist_ok=True)
    pages = crawl()
    with open("data/site_pages.json", "w", encoding="utf-8") as f:
        json.dump(pages, f, ensure_ascii=False, indent=2)
    print(f"âœ… Crawled pages: {len(pages)}")
    print("ğŸ‘‰ Saved to data/site_pages.json")


    '''
    æ•´ä½“æ€è·¯ï¼š
    queueï¼šå…ˆæ”¾å…¥base url+é‚£å‡ ä¸ªå¤§ç±»
    if queue + ä¸è¶…è¿‡page numberï¼š
    æ£€æŸ¥depthæ˜¯å¦åœ¨èŒƒå›´å†…
        ï¼ˆä¿è¯æ˜¯æ­£ç¡®çš„htmlåï¼‰æŠ“å»æ­£æ–‡+æ ‡è®°ä¸ºseen
        æ‰¾href readmoreçš„æ’é˜Ÿ å…¶ä»–çš„æ­£å¸¸æ”¾å…¥é˜Ÿåˆ—ï¼ˆBFSï¼‰è¿™äº›depthéƒ½è¦+1

    '''